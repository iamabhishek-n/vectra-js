const path = require('path');
const { RAGClient, ProviderType, ChunkingStrategy, RetrievalStrategy } = require('./index');

// --- MOCK CLASSES ---

// Mock Prisma Client to avoid DB connection requirements
class MockPrismaClient {
    async $executeRawUnsafe(query, ...params) {
        console.log(`[MockDB] Insert: ${query.substring(0, 50)}... Params: [${params.length}]`);
        return 1;
    }
    async $queryRawUnsafe(query, ...params) {
        console.log(`[MockDB] Query: ${query.substring(0, 50)}...`);
        // Return a fake search result
        return [{
            content: "RAG combines LLMs with external knowledge.",
            metadata: { source: "mock", chunkIndex: 0 },
            score: 0.89
        }];
    }
}

// Main Test Function
async function runSimulation() {
    console.log("=== Starting Vectra SDK Simulation (Node.js) ===\n");

    // 1. Configuration
    const config = {
        embedding: {
            provider: ProviderType.OPENAI,
            apiKey: "mock-key",
            modelName: "text-embedding-3-small"
        },
        llm: {
            provider: ProviderType.GEMINI, 
            apiKey: "mock-key",
            modelName: "gemini-1.5-flash"
        },
        chunking: {
            strategy: ChunkingStrategy.RECURSIVE,
            chunkSize: 100,
            chunkOverlap: 20
        },
        database: {
            type: 'prisma',
            tableName: 'Document',
            columnMap: { content: 'content', vector: 'vector', metadata: 'metadata' },
            clientInstance: new MockPrismaClient() // Updated to use clientInstance
        },
        retrieval: {
            strategy: RetrievalStrategy.HYBRID // Testing Hybrid logic
        },
        callbacks: [{
            onRetrievalStart: (q) => console.log(`[Event] Retrieving for: "${q}"`),
            onGenerationStart: (p) => console.log(`[Event] Generating answer...`),
        }]
    };

    console.log("Initializing Client...");
    const client = new RAGClient(config);

    // --- MONKEY PATCHING FOR SIMULATION ---
    
    client.embedder.embedDocuments = async (texts) => {
        return texts.map(() => Array(10).fill(0).map(() => Math.random()));
    };
    client.embedder.embedQuery = async (text) => {
        return Array(10).fill(0).map(() => Math.random());
    };

    client.llm.generate = async (prompt) => {
        return "This is a simulated answer generated by the RAG SDK test script.";
    };
    
    // Mock Streaming
    client.llm.generateStream = async function* (prompt) {
        const words = ["This", " is", " a", " streamed", " response."];
        for (const w of words) {
            await new Promise(r => setTimeout(r, 100)); // Simulate delay
            yield w;
        }
    };

    // 2. Query (Standard)
    console.log(`\n--- Step 1: Standard Query (Hybrid) ---
`);
    try {
        const result = await client.queryRAG("What is RAG?");
        console.log("Answer:", result.answer);
    } catch (error) {
        console.error("Query failed:", error);
    }
    
    // 3. Query (Streaming)
    console.log(`\n--- Step 2: Streaming Query ---
`);
    try {
        const stream = await client.queryRAG("Tell me more...", null, true);
        process.stdout.write("Stream Output: ");
        for await (const chunk of stream) {
            process.stdout.write(chunk);
        }
        console.log("\n");
    } catch (error) {
        console.error("Streaming failed:", error);
    }
}

runSimulation();